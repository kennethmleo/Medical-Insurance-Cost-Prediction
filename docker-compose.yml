services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.5.1
    container_name: mlflow-server
    ports:
      - "5500:5500"
    environment:
      # ✅ Filesystem-based tracking (same as your train.py)
      - MLFLOW_TRACKING_URI=file:/mlruns
    volumes:
      # ✅ Shared volume for experiment runs, artifacts, and models
      - ./mlruns:/mlruns
    command: >
      mlflow server
      --backend-store-uri file:/mlruns
      --default-artifact-root /mlruns
      --host 0.0.0.0
      --port 5500
      --serve-artifacts
      --cors-allowed-origins=*
      --allowed-hosts=*

  predictor:
    build: .
    container_name: insurance-predictor
    depends_on:
      - mlflow
    ports:
      - "9696:9696"
    environment:
      # ✅ Points to the mlflow service via Docker network
      - MLFLOW_TRACKING_URI=http://mlflow:5500
    volumes:
      # ✅ Must share the same mlruns volume so it sees artifacts
      - ./mlruns:/mlruns
    command: >
      python src/service/predict_flask.py